{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "Export PyTorch AnimeGANv2 → ONNX → TensorFlow SavedModel → TF.js + tflite",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Tt7RAS-RH2"
      },
      "source": [
        "!pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
        "!pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PtfVT0SeWOR"
      },
      "source": [
        "import io\n",
        "import os, sys\n",
        "import requests\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "from IPython.display import display, display_markdown\n",
        "\n",
        "\n",
        "\n",
        "def download_image(url):\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    return PIL.Image.open(io.BytesIO(resp.content))\n",
        "\n",
        "# target_image_size = 1024\n",
        "# def preprocess(img):\n",
        "#     s = min(img.size)\n",
        "    \n",
        "#     if s < target_image_size:\n",
        "#         raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
        "        \n",
        "#     r = target_image_size / s\n",
        "#     s = (round(r * img.size[1]), round(r * img.size[0]))\n",
        "#     img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
        "#     img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
        "#     img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
        "#     mapped = map_pixels(img)\n",
        "#     return mapped"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33dNgzBaw2vD"
      },
      "source": [
        "# img = download_image('https://i.imgur.com/ayV3L3O.jpg') #1024x1024\n",
        "img = download_image('https://i.imgur.com/hFUANgu.jpg') #512x512"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-blVzqZeWOY"
      },
      "source": [
        "import os\n",
        "import collections\n",
        "from typing import Union, List\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import PIL.Image\n",
        "import PIL.ImageFile\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "\n",
        "import os\n",
        "os.system(\"git clone https://github.com/bryandlee/animegan2-pytorch\")\n",
        "os.system(\"gdown https://drive.google.com/uc?id=1WK5Mdt6mwlcsqCZMHkCUSDJxN1UyFi0-\")\n",
        "os.system(\"gdown https://drive.google.com/uc?id=18H3iK09_d54qEDoWIc82SyWB2xun4gjU\")\n",
        "import sys\n",
        "sys.path.append(\"animegan2-pytorch\")\n",
        "import torch\n",
        "# torch.set_grad_enabled(False)\n",
        "from model import Generator\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Generator().eval().to(device)\n",
        "model.load_state_dict(torch.load(\"face_paint_512_v2_0.pt\"))\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "\n",
        "input = to_tensor(img).unsqueeze(0) * 2 - 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utucRWI55sOJ",
        "outputId": "22450f25-4a38-404c-b51e-82f0a72ed5d5"
      },
      "source": [
        "print(input)\n",
        "output = model(input.to(device)).cpu()[0]\n",
        "print(output)\n",
        "output = (output * 0.5 + 0.5).clip(0, 1)\n",
        "print(output)\n",
        "out_img = to_pil_image(output)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.6392, -0.6392, -0.6392,  ..., -0.3647, -0.3647, -0.3647],\n",
            "          [-0.6392, -0.6392, -0.6392,  ..., -0.3647, -0.3647, -0.3647],\n",
            "          [-0.6392, -0.6392, -0.6392,  ..., -0.3647, -0.3647, -0.3647],\n",
            "          ...,\n",
            "          [ 0.4510,  0.4510,  0.4510,  ...,  0.5373,  0.5451,  0.5608],\n",
            "          [ 0.5059,  0.5059,  0.5059,  ...,  0.5373,  0.5451,  0.5608],\n",
            "          [ 0.5373,  0.5373,  0.5373,  ...,  0.5373,  0.5451,  0.5608]],\n",
            "\n",
            "         [[-0.0275, -0.0275, -0.0275,  ...,  0.1137,  0.1137,  0.1137],\n",
            "          [-0.0275, -0.0275, -0.0275,  ...,  0.1137,  0.1137,  0.1137],\n",
            "          [-0.0275, -0.0275, -0.0275,  ...,  0.1137,  0.1137,  0.1137],\n",
            "          ...,\n",
            "          [ 0.5294,  0.5294,  0.5294,  ...,  0.5608,  0.5686,  0.5843],\n",
            "          [ 0.5843,  0.5843,  0.5843,  ...,  0.5608,  0.5686,  0.5843],\n",
            "          [ 0.6157,  0.6157,  0.6157,  ...,  0.5608,  0.5686,  0.5843]],\n",
            "\n",
            "         [[ 0.5529,  0.5529,  0.5529,  ...,  0.6392,  0.6392,  0.6392],\n",
            "          [ 0.5529,  0.5529,  0.5529,  ...,  0.6392,  0.6392,  0.6392],\n",
            "          [ 0.5529,  0.5529,  0.5529,  ...,  0.6392,  0.6392,  0.6392],\n",
            "          ...,\n",
            "          [ 0.6235,  0.6235,  0.6235,  ...,  0.6157,  0.6235,  0.6392],\n",
            "          [ 0.6784,  0.6784,  0.6784,  ...,  0.6157,  0.6235,  0.6392],\n",
            "          [ 0.7098,  0.7098,  0.7098,  ...,  0.6157,  0.6235,  0.6392]]]])\n",
            "tensor([[[-0.7186, -0.7175, -0.7188,  ..., -0.4813, -0.4829, -0.4863],\n",
            "         [-0.7191, -0.7170, -0.7164,  ..., -0.4797, -0.4800, -0.4839],\n",
            "         [-0.7191, -0.7187, -0.7170,  ..., -0.4836, -0.4809, -0.4839],\n",
            "         ...,\n",
            "         [ 0.0872,  0.0863,  0.0680,  ...,  0.2753,  0.2616,  0.2127],\n",
            "         [ 0.1246,  0.1248,  0.1050,  ...,  0.2779,  0.2665,  0.2153],\n",
            "         [ 0.1353,  0.1390,  0.1270,  ...,  0.2764,  0.2688,  0.2094]],\n",
            "\n",
            "        [[-0.0207, -0.0187, -0.0167,  ...,  0.1122,  0.1110,  0.1074],\n",
            "         [-0.0220, -0.0181, -0.0132,  ...,  0.1145,  0.1157,  0.1116],\n",
            "         [-0.0239, -0.0217, -0.0162,  ...,  0.1114,  0.1159,  0.1124],\n",
            "         ...,\n",
            "         [ 0.1943,  0.1935,  0.1832,  ...,  0.2472,  0.2415,  0.2082],\n",
            "         [ 0.2186,  0.2195,  0.2081,  ...,  0.2479,  0.2436,  0.2106],\n",
            "         [ 0.2234,  0.2293,  0.2222,  ...,  0.2439,  0.2440,  0.2031]],\n",
            "\n",
            "        [[ 0.4167,  0.4172,  0.4181,  ...,  0.5403,  0.5398,  0.5375],\n",
            "         [ 0.4178,  0.4197,  0.4230,  ...,  0.5434,  0.5449,  0.5423],\n",
            "         [ 0.4169,  0.4176,  0.4213,  ...,  0.5415,  0.5459,  0.5439],\n",
            "         ...,\n",
            "         [ 0.3497,  0.3518,  0.3452,  ...,  0.2800,  0.2766,  0.2486],\n",
            "         [ 0.3690,  0.3716,  0.3629,  ...,  0.2810,  0.2781,  0.2516],\n",
            "         [ 0.3737,  0.3809,  0.3742,  ...,  0.2761,  0.2778,  0.2441]]],\n",
            "       grad_fn=<SelectBackward>)\n",
            "tensor([[[0.1407, 0.1412, 0.1406,  ..., 0.2593, 0.2585, 0.2569],\n",
            "         [0.1405, 0.1415, 0.1418,  ..., 0.2602, 0.2600, 0.2580],\n",
            "         [0.1405, 0.1407, 0.1415,  ..., 0.2582, 0.2596, 0.2581],\n",
            "         ...,\n",
            "         [0.5436, 0.5432, 0.5340,  ..., 0.6377, 0.6308, 0.6063],\n",
            "         [0.5623, 0.5624, 0.5525,  ..., 0.6389, 0.6333, 0.6076],\n",
            "         [0.5677, 0.5695, 0.5635,  ..., 0.6382, 0.6344, 0.6047]],\n",
            "\n",
            "        [[0.4896, 0.4907, 0.4917,  ..., 0.5561, 0.5555, 0.5537],\n",
            "         [0.4890, 0.4910, 0.4934,  ..., 0.5573, 0.5579, 0.5558],\n",
            "         [0.4881, 0.4891, 0.4919,  ..., 0.5557, 0.5579, 0.5562],\n",
            "         ...,\n",
            "         [0.5971, 0.5967, 0.5916,  ..., 0.6236, 0.6207, 0.6041],\n",
            "         [0.6093, 0.6097, 0.6041,  ..., 0.6240, 0.6218, 0.6053],\n",
            "         [0.6117, 0.6146, 0.6111,  ..., 0.6219, 0.6220, 0.6015]],\n",
            "\n",
            "        [[0.7083, 0.7086, 0.7090,  ..., 0.7701, 0.7699, 0.7688],\n",
            "         [0.7089, 0.7098, 0.7115,  ..., 0.7717, 0.7725, 0.7712],\n",
            "         [0.7085, 0.7088, 0.7106,  ..., 0.7708, 0.7730, 0.7719],\n",
            "         ...,\n",
            "         [0.6749, 0.6759, 0.6726,  ..., 0.6400, 0.6383, 0.6243],\n",
            "         [0.6845, 0.6858, 0.6814,  ..., 0.6405, 0.6390, 0.6258],\n",
            "         [0.6869, 0.6904, 0.6871,  ..., 0.6381, 0.6389, 0.6221]]],\n",
            "       grad_fn=<ClampBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MHoJt7qzb9_"
      },
      "source": [
        "display(out_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZj_UfKWeWOd"
      },
      "source": [
        "torch.onnx.export(model, input.to(device), \"anime-gan-v2.onnx\", export_params=True, opset_version=12) "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYFMdrNRpOLf"
      },
      "source": [
        "!onnx-tf convert -i anime-gan-v2.onnx -o anime-gan-v2-tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXuUSBSTq1Sn"
      },
      "source": [
        "!tensorflowjs_converter --input_format tf_saved_model ./anime-gan-v2-tf ./anime-gan-v2-tfjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaUpEHpJ5SFX",
        "outputId": "5305c78e-f054-4d1b-a923-567588a201cc"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert the model to tflite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"./anime-gan-v2-tf\") # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('anime-gan-v2.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    }
  ]
}
